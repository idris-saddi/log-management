% Internship Report - Log Management System
\documentclass[12pt,a4paper]{report}

% Page layout and packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{array}
\usepackage{longtable}
\usepackage{pgfgantt}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{float}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{fancyvrb}
\usepackage{verbatim}

% Geometry and spacing
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\onehalfspacing

% Hyperref setup
\hypersetup{colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue}

% Unicode support for verbatim
\usepackage[utf8]{inputenc}
\usepackage{newunicodechar}
\newunicodechar{âœ…}{[OK]}
\newunicodechar{âŒ}{[ERROR]}
\newunicodechar{ðŸ•’}{[CLOCK]}
\newunicodechar{ðŸ”Œ}{[PLUG]}
\newunicodechar{ðŸ“¦}{[PACKAGE]}
\newunicodechar{ðŸ”}{[REPEAT]}
\newunicodechar{ðŸ“Š}{[CHART]}
\newunicodechar{ðŸš¨}{[ALARM]}
\newunicodechar{âš }{[WARNING]}
\newunicodechar{ï¸}{}
\newunicodechar{ðŸ“ˆ}{[TREND]}
\newunicodechar{â±}{[TIMER]}
\newunicodechar{ðŸš€}{[ROCKET]}
\newunicodechar{ðŸ”}{[SEARCH]}
\newunicodechar{ðŸ”§}{[TOOL]}
\newunicodechar{â„¹}{[INFO]}
\newunicodechar{ðŸ“¢}{[ANNOUNCE]}
\newunicodechar{ðŸ”—}{[LINK]}
\newunicodechar{ðŸŽ‰}{[PARTY]}

% Title page
\begin{document}
\begin{titlepage}
    \centering
    {\Large National Institute of Applied Sciences and Technology (INSAT)\\[0.25cm]}
    {\large Computer Science Department\\[1.2cm]}
    {\LARGE \textbf{Internship Report}}\\[0.6cm]
    {\Large \textbf{Centralized Log Management System for Microservices}}\\[1.2cm]

  \begin{tabular}{>{\bfseries}p{5cm} p{9cm}}
    Student: & Idris SADDI \\
    Company: & MedSirat \\
    Internship period: & 01/07/2025 -- 31/08/2025 \\
    Supervisor: & Salma SEDDIK \\
    Academic advisor: & INSAT \\
  \end{tabular}

    \vfill
    % Optional logo
    % \includegraphics[width=0.35\textwidth]{logo-insat}

    \vfill
    \large \textit{Submission date: September 23, 2025}
\end{titlepage}

% Acknowledgements
\chapter*{Acknowledgements}
I would like to express my sincere gratitude to my company supervisor, the engineering team, and all colleagues for their support during this internship. Their guidance and trust were instrumental to the success of this project. I also thank my professors at INSAT for their academic support and the knowledge they provided.

\pagenumbering{roman}
\tableofcontents
\listoffigures
\listoftables
\clearpage
\pagenumbering{arabic}

% Abstract
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
This report presents the design and implementation of a \textbf{centralized log management system} for a microservices environment. The primary objective was to \textbf{collect}, \textbf{aggregate}, \textbf{analyze}, and \textbf{monitor} application logs reliably and at scale. The solution is built with \textit{Spring Boot 3.5.3}, \textit{Apache Kafka}, and \textit{Graylog 6.3}, orchestrated via \textit{Docker Compose}. A dedicated \textit{log-service} consumes messages from the Kafka topic \texttt{logs} and forwards them to Graylog using \textit{GELF TCP}. The initialization script \texttt{graylog-init.sh} automates the creation of \textit{inputs}, \textit{streams}, \textit{dashboards}, and \textit{alerts}. The results show a \textbf{reduction in operational effort} and an \textbf{improvement in service observability}.

\clearpage
\chapter*{General Introduction}
\addcontentsline{toc}{chapter}{General Introduction}
Modern distributed systems generate high volumes of heterogeneous logs across multiple services and infrastructure layers. Without centralization and structure, troubleshooting becomes slow and error-prone. This internship addresses these challenges by implementing a production-ready logging platform that enables end-to-end visibility, proactive alerting, and operational insights with minimal manual configuration.

\chapter{Company Context and Objectives}
\section{Host Company Overview}
% Replace with factual information about the host company (max 2 pages)
Industry: \textit{to be completed}. Annual revenue: \textit{to be completed}. Organization chart: \textit{to be completed}. Certifications: \textit{ISO 9001, ISO 27001 if applicable}. Engineering staff ratio: \textit{to be completed}.

\section{Internship Context and Problem Statement}
The microservices architecture used by the company required a robust, centralized approach to log collection and analysis. Disparate logs across services were slowing incident resolution and hindering monitoring. The challenge was to implement a scalable pipeline that ingests logs asynchronously, enriches them with context, and renders them searchable and actionable in near real-time.

\section{Objectives}
The internship objectives were as follows:
\begin{itemize}[leftmargin=1.2cm]
    \item Design a \textbf{centralized log ingestion architecture}.
    \item Implement a \textbf{reliable asynchronous transport} using Kafka.
    \item Integrate \textbf{GELF} (Graylog Extended Log Format) with MDC enrichment.
    \item Automate \textbf{Graylog provisioning} (inputs, streams, dashboards, alerts).
    \item Deliver a \textbf{containerized deployment} with reproducible environments.
\end{itemize}

\section{Project Plan (Gantt)}
The following Gantt chart summarizes the project plan.

\begin{figure}[H]
\centering
\begin{ganttchart}[
    hgrid, vgrid,
    y unit chart=0.7cm,
    x unit=0.65cm,
    time slot format=isodate
]{2025-07-01}{2025-09-15}
    \gantttitlecalendar{year, month=name, week} \\
    % Phases
    \ganttgroup{Initiation}{2025-07-01}{2025-07-14} \\
    \ganttbar{Requirements \\ \/ Scoping}{2025-07-01}{2025-07-07} \\
    \ganttbar{Docker Environment}{2025-07-05}{2025-07-14} \\
    \ganttgroup{Implementation}{2025-07-15}{2025-08-25} \\
    \ganttbar{Spring Boot Services}{2025-07-15}{2025-07-31} \\
    \ganttbar{Kafka \texttt{logs} Topic}{2025-07-20}{2025-08-05} \\
    \ganttbar{Log Service (Consumer/GELF)}{2025-07-25}{2025-08-10} \\
    \ganttbar{Graylog Automation}{2025-08-05}{2025-08-20} \\
    \ganttgroup{Validation}{2025-08-26}{2025-09-10} \\
    \ganttbar{Integration Tests}{2025-08-26}{2025-09-05} \\
    \ganttbar{Dashboards \\ \& Alerts Tuning}{2025-09-01}{2025-09-10}
\end{ganttchart}
\caption{Project Gantt chart}
\end{figure}

\chapter{System Overview and Requirements}
\section{Functional Requirements}
\begin{itemize}[leftmargin=1.2cm]
  \item Collect logs from multiple services with minimal coupling.
  \item Support multiple log levels and structured fields.
  \item Provide search, dashboards, and alerts for operations.
  \item Automate provisioning to reduce manual setup.
\end{itemize}

\section{Non-Functional Requirements}
\begin{itemize}[leftmargin=1.2cm]
  \item Scalability via asynchronous message transport (Kafka).
  \item Resilience to partial outages (backpressure, retries).
  \item Security of transport and access control (discussed in Chapter~\ref{chap:security}).
  \item Observability and maintainability for SRE workflows.
\end{itemize}

\chapter{Background and Related Work}
Log management in distributed systems builds on decades of work in observability. Traditional centralized approaches relied on syslog; modern stacks include ELK/OS (Elasticsearch/OpenSearch, Logstash, Kibana) and Graylog. Kafka-based pipelines decouple producers from consumers, increasing resilience. Related work explores schema-on-write vs. schema-on-read trade-offs, log sampling, and privacy-preserving logging.

\section{Positioning of This Work}
This project emphasizes operational automation (Graylog provisioning) and reproducible environments to reduce setup costs. It demonstrates an end-to-end pattern from application logging to dashboards and alerts using open-source components.

\chapter{Methodology}
\section{Process Model}
An incremental and iterative process was used (adapted Scrum): bi-weekly iterations with planning, implementation, review, and retrospective. Each iteration delivered a testable increment (e.g., service endpoints, Kafka ingestion, init scripts, dashboards).

\section{Tools and Practices}
Version control with Git, issue tracking via GitHub Issues, code reviews for critical changes, and containerized dev environments ensured consistency. Observability was validated with synthetic traffic and exploratory testing in Graylog.

\section{Week 1: Technology Survey and Selection}
In the first week, I conducted a focused survey of log management stacks: \textbf{ELK (Elasticsearch/OpenSearch + Logstash + Kibana)}, \textbf{Graylog}, and \textbf{Loki}.
\begin{itemize}[leftmargin=1.2cm]
  \item \textbf{ELK/OS}: very flexible, large ecosystem; Logstash/Beats for ingestion; Kibana dashboards. Pros: mature, powerful queries and visualizations. Cons: more moving parts; provisioning and pipeline config are heavier for our needs.
  \item \textbf{Graylog}: opinionated platform with built-in inputs, streams, dashboards, and alerts; strong API for automation. Pros: fast to value, centralized UI, manageable provisioning via REST; native GELF. Cons: feature set aligned to logging (less general analytics than Kibana for non-log data).
  \item \textbf{Loki}: optimized for label-based indexing and log streaming, tight Grafana integration. Pros: cost-efficient for large volumes, great with Grafana. Cons: query model differs (LogQL), and our pipeline required stronger message routing/alerting primitives out-of-the-box.
\end{itemize}
After evaluating the fit for \textit{our objectives} (rapid automation, streams/routing, alerting, reproducible local stack), we chose \textbf{Graylog}. This decision enabled us to automate inputs/streams/dashboards/alerts through a single script and leverage GELF TCP with MDC for structured logs without a heavy Logstash layer.

\subsection*{Comparison Matrix}
\begin{longtable}{p{3.2cm} p{3.8cm} p{3.8cm} p{3.8cm}}
\toprule
\textbf{Feature} & \textbf{ELK / OpenSearch Stack} & \textbf{Graylog} & \textbf{Loki} \\
\midrule
\endhead
Ingestion & Logstash/Beats; flexible pipelines & Built-in inputs (e.g., GELF TCP) & Promtail agents; label-based \\
Storage/Index & Elasticsearch/OpenSearch & OpenSearch via Graylog & Chunked object store + index \\
Query language & Lucene/KQL & Lucene via Graylog UI & LogQL (labels + regex) \\
Dashboards & Kibana & Built-in dashboards & Grafana \\
Alerts & Kibana/Watchers/Alerting & Native event/notification & Grafana alerting \\
Provisioning & APIs + Kibana objects; more moving parts & Unified REST API; streams/dashboards/alerts in one place & Grafana/Loki APIs; separate components \\
Operational complexity & Higher (more components) & Moderate (opinionated) & Moderate (Grafana+Loki+Promtail) \\
Best fit & Broad analytics, complex pipelines & Logging-centric with routing/alerts & Cost-efficient log streaming with Grafana \\
\bottomrule
\end{longtable}

\chapter{Requirements Engineering}
\section{Elicitation and Analysis}
Requirements were gathered from SRE and development stakeholders focusing on operational pain points (slow troubleshooting, inconsistent logging). The following MoSCoW prioritization was established.

\begin{table}[H]
\centering
\begin{tabular}{p{3cm} p{11cm}}
\toprule
\textbf{Priority} & \textbf{Requirement} \\
\midrule
Must Have & Centralized log search; ERROR alerting; automated provisioning; containerized deployment \\
Should Have & Level distribution dashboard; MDC enrichment; response time metrics \\
Could Have & HTTP notifications; service-specific dashboards; top sources table \\
Won't Have & Distributed tracing (deferred) \\
\bottomrule
\end{tabular}
\caption{MoSCoW prioritization}
\end{table}

\section{Traceability}
Each requirement mapped to concrete deliverables (scripts, configs, dashboards), captured in repository commits and README documentation.

\chapter{Architecture}
\section{Logical Architecture}
The platform consists of two application services (\texttt{service1}, \texttt{service2}) publishing JSON logs to Kafka, a dedicated \texttt{log-service} consuming the \texttt{logs} topic and forwarding GELF messages to Graylog, and the Graylog stack backed by OpenSearch (for data) and MongoDB (for configuration).

\section{Component Responsibilities}
\begin{itemize}[leftmargin=1.2cm]
  \item \textbf{service1/2}: expose a REST endpoint \texttt{/log} to publish log events into Kafka.
  \item \textbf{Kafka/Zookeeper}: reliable asynchronous transport for log messages.
  \item \textbf{log-service}: Kafka consumer, MDC enrichment, GELF TCP forwarding.
  \item \textbf{Graylog}: ingestion, indexing (OpenSearch), dashboards, alerting, search.
\end{itemize}

\section{Design Rationale}
Kafka ensures asynchronous decoupling to mitigate backpressure. GELF provides structured transport with MDC support, simplifying parsing and indexing. Graylog was selected for its API-first provisioning and built-in alerts.

\section{Sequence of Interactions}
1) Client requests to service endpoint. 2) Service publishes log to Kafka (topic \texttt{logs}). 3) Log-service consumes and enriches. 4) GELF TCP to Graylog. 5) Graylog indexes into OpenSearch and manages dashboards/alerts.

\section{Deployment Topology}
\section{End-to-End Data Flow}
\begin{enumerate}
  \item Application emits a structured JSON event using Logback JSON encoder (with a static \texttt{"service"} field derived from \texttt{spring.application.name}).
  \item Logback Kafka Appender publishes the event to Kafka topic \texttt{logs} using the Spring property \texttt{spring.kafka.bootstrap-servers=kafka:9092}.
  \item The \texttt{log-service} Kafka consumer (group \texttt{log-consumers}) receives the event, parses it into a \texttt{LogMessage} DTO (tolerant to extra fields), and enriches MDC with \texttt{service}, \texttt{originalTS}, and normalized level fields.
  \item Logback in \texttt{log-service} forwards the enriched record via GELF TCP to Graylog on \texttt{graylog:12201}.
  \item Graylog persists messages in OpenSearch and stores configuration in MongoDB. The init script creates streams, dashboards, and alert definitions per service.
\end{enumerate}

\section{Integration Points and Interfaces}
\begin{itemize}[leftmargin=1.2cm]
  \item \textbf{Kafka contract}: topic name \texttt{logs}, value is JSON string (StringSerializer). No key is required by current producers.
  \item \textbf{Consumer DTO}: fields accepted are \texttt{@timestamp|timestamp}, \texttt{message}, \texttt{level}, \texttt{service}, optional \texttt{thread\_name}, \texttt{logger\_name}, \texttt{stack\_trace}. Unknown fields are ignored.
  \item \textbf{Graylog input}: GELF TCP global input on port \texttt{12201} (no TLS in local dev). Streams route by field \texttt{service}.
  \item \textbf{Dashboards/alerts}: created via REST API by \texttt{graylog-init.sh}; title and widgets are deterministic per service list.
\end{itemize}

\section{Strengths and Limitations}
\subsection*{Strengths}
\begin{itemize}[leftmargin=1.2cm]
  \item \textbf{Loose coupling via Kafka}: producers are decoupled from consumers; the pipeline tolerates bursts and partial outages.
  \item \textbf{Structured logging}: consistent JSON and MDC enable efficient search and aggregations in Graylog.
  \item \textbf{Automation}: end-to-end Graylog provisioning removes manual steps and drift.
  \item \textbf{Reproducible environment}: Docker Compose sets up the full stack predictably for development/testing.
\end{itemize}

\subsection*{Limitations}
\begin{itemize}[leftmargin=1.2cm]
  \item \textbf{Single-broker dev setup}: the compose file uses a single Kafka broker and single-node OpenSearch; not fault tolerant.
  \item \textbf{No transport encryption in dev}: GELF TCP and Graylog UI are not TLS-enabled by default in local environment.
  \item \textbf{Basic schema}: the minimal event schema lacks correlation IDs and rich context (e.g., user/session) unless added by producers.
  \item \textbf{Backpressure visibility}: without explicit metrics, diagnosing dropped or delayed messages requires additional observability.
\end{itemize}

\section{Scalability Considerations}
\begin{itemize}[leftmargin=1.2cm]
  \item Increase Kafka partitions for \texttt{logs} and run multiple \texttt{log-service} consumers in the same group.
  \item Use Graylog inputs behind load balancers and scale OpenSearch with multiple data nodes and index tuning.
  \item Introduce schemas (Avro/Protobuf) and a schema registry for stronger contracts as the event model evolves.
\end{itemize}
\section{Technology Stack Details}
\subsection{Runtime and Frameworks}
\begin{itemize}[leftmargin=1.2cm]
  \item Java 21 (LTS) with Spring Boot 3.5.3 across all services.
  \item Spring Kafka for producer/consumer integration.
  \item Logback with \texttt{logstash-logback-encoder} (v7.3) for structured JSON events.
  \item Logback Kafka Appender (service1) \texttt{com.github.danielwegener:logback-kafka-appender:0.2.0-RC2}.
  \item GELF TCP appender (log-service) \texttt{de.siegmar:logback-gelf:4.0.2} and \texttt{org.graylog2:gelfclient:1.5.1} available.
\end{itemize}

\subsection{Messaging and Storage}
\begin{itemize}[leftmargin=1.2cm]
  \item Kafka (\texttt{wurstmeister/kafka}) with Zookeeper (\texttt{confluentinc/cp-zookeeper:7.4.3}).
  \item Graylog 6.3 with OpenSearch 2.x (single-node, security disabled for local) and MongoDB 6.0.5.
\end{itemize}

\subsection{Configuration Summary}
Key properties from \texttt{application.properties} files:
\begin{itemize}[leftmargin=1.2cm]
  \item Producers (service1, service2): \texttt{spring.kafka.bootstrap-servers=kafka:9092}; String serializers.
  \item Consumer (log-service): group \texttt{log-consumers}, \texttt{auto-offset-reset=earliest}, String deserializers.
  \item log-service port: 8083; service1: 8081; service2: 8082.
\end{itemize}

\section{Log Event Schema and Enrichment}
\subsection{Producer-side JSON}
Services publish JSON events containing at least \texttt{timestamp}, \texttt{message}, \texttt{level}, and \texttt{service} (see Figure~\ref{fig:service-json}).

\begin{figure}[H]
\centering
\begin{Verbatim}[frame=single, fontsize=\small, xleftmargin=1cm, xrightmargin=1cm]
{
  "timestamp": "2025-07-15T10:45:12.345Z",
  "message": "UserAuthenticated",
  "level": "INFO",
  "service": "service1"
}
\end{Verbatim}
\caption{Minimal producer event structure (service1/service2)}
\label{fig:service-json}
\end{figure}

\subsection{Consumer DTO Mapping}
The Kafka consumer maps messages to \texttt{LogMessage} with optional fields ignored (\texttt{@JsonIgnoreProperties(ignoreUnknown=true)}). Supported fields:
\begin{itemize}[leftmargin=1.2cm]
  \item \texttt{@timestamp} (string), \texttt{message}, \texttt{level}, \texttt{service},
  \item \texttt{thread\_name}, \texttt{logger\_name}, \texttt{stack\_trace}.
\end{itemize}

\subsection{MDC Enrichment and Routing}
During processing, the consumer sets MDC keys: \texttt{service}, \texttt{originalTS}, \texttt{levelName}, and \texttt{level}. Logback GELF encoder includes MDC, enabling routing and dashboards in Graylog based on \texttt{service} and \texttt{level}.

\section{Configuration Matrix}
\begin{table}[H]
\centering
\begin{tabular}{p{4cm} p{4cm} p{7cm}}
\toprule
\textbf{Component} & \textbf{Key Setting} & \textbf{Value / Purpose} \\
\midrule
service1/service2 & Kafka bootstrap & \texttt{kafka:9092} (container DNS) \\
service1/service2 & Kafka serializers & String/JSON via logback encoder \\
log-service & Consumer group & \texttt{log-consumers} (scalable) \\
log-service & GELF target & \texttt{graylog:12201} (TCP) \\
Graylog & Input & GELF TCP global input (scripted) \\
Graylog & Streams & Per-service stream with rule \texttt{service=...} \\
OpenSearch & Single-node & Dev mode, security disabled (local) \\
\bottomrule
\end{tabular}
\caption{Configuration matrix across components}
\end{table}
All components are orchestrated with Docker Compose on a single bridge network ensuring service discovery via container names.

\begin{table}[H]
\centering
\begin{tabular}{llp{7cm}}
\toprule
\textbf{Service} & \textbf{Port} & \textbf{Purpose} \\
\midrule
Graylog UI & 9000 & Web interface for log management \\
OpenSearch & 9200 & Index and search backend \\
Service 1 & 8081 & Example microservice with logging endpoints \\
Service 2 & 8082 & Example microservice with logging endpoints \\
Log Service & 8083 & Kafka consumer and GELF forwarder \\
Kafka & 9092 & Message broker \\
Zookeeper & 2181 & Kafka coordination \\
GELF TCP & 12201 & Graylog input for log ingestion \\
\bottomrule
\end{tabular}
\caption{Deployed services and ports}
\end{table}

\chapter{Implementation}
\section{Service Endpoints and Log Publishing}
The services provide a REST endpoint that accepts a message and level, normalizes the level, serializes to JSON, and publishes to Kafka.

\begin{figure}[H]
\centering
\begin{Verbatim}[frame=single, fontsize=\small, xleftmargin=1cm, xrightmargin=1cm]
// See Appendix C for the full source
\end{Verbatim}
\caption{Excerpt: service1 LogController (publishing to Kafka)}
\label{fig:logcontroller}
\end{figure}

\section{Kafka Consumer and GELF Forwarder}
The log-service consumes the \texttt{logs} topic and forwards to Graylog using a GELF TCP appender configured in Logback.

\begin{figure}[H]
\centering
\begin{Verbatim}[frame=single, fontsize=\small, xleftmargin=1cm, xrightmargin=1cm]
// See Appendix D for the full source
\end{Verbatim}
\caption{Excerpt: log-service Kafka consumer}
\label{fig:logconsumer}
\end{figure}

\section{Graylog Automation}
The \texttt{graylog-init.sh} script waits for the Graylog API, creates the global GELF TCP input, configures streams per service with routing rules, generates searches and dashboards with multiple widgets, and adds an ERROR-level alert with an HTTP notification.

\begin{figure}[H]
\centering
\begin{Verbatim}[frame=single, fontsize=\small, xleftmargin=1cm, xrightmargin=1cm]
1) Wait for Graylog API readiness
2) Create global GELF TCP input on port 12201
3) For each service: create stream, rules, search, dashboard
4) Add widgets (counts, level distribution, timeline, sources)
5) Create ERROR alert and attach HTTP notification
\end{Verbatim}
\caption{Automation script responsibilities}
\label{fig:initoverview}
\end{figure}

\chapter{Configuration and Deployment}
\section{Docker Compose}
The environment is fully orchestrated via Docker Compose with persistent volumes and a dedicated network for internal communication.

\begin{figure}[H]
\centering
\begin{Verbatim}[frame=single, fontsize=\small, xleftmargin=1cm, xrightmargin=1cm]
# See Appendix A for the full file
\end{Verbatim}
\caption{docker-compose.yml overview}
\label{fig:compose}
\end{figure}

\section{Logging Configuration}
GELF TCP is configured in Logback to enrich messages with MDC and send them to Graylog.

\begin{figure}[H]
\centering
\begin{Verbatim}[frame=single, fontsize=\small, xleftmargin=1cm, xrightmargin=1cm]
<!-- See Appendix E for the full file -->
\end{Verbatim}
\caption{Logback GELF appender (log-service)}
\label{fig:logback}
\end{figure}

\section{Running Locally}
Prerequisites: Docker, Docker Compose, Java 21, Maven, \texttt{jq}, and \texttt{curl}.
\begin{enumerate}
  \item Build and start: \texttt{docker compose up -d --build}
  \item Wait for Graylog init container to finish
  \item Test endpoints: POST/GET \texttt{/log} on services 8081 and 8082
\end{enumerate}

\chapter{Testing Strategy and Validation}
\section{Test Plan}
Test categories included unit (service-level logic), integration (Kafka publish/consume), system (end-to-end log visibility in Graylog), and acceptance (dashboards/alerts meet requirements).

\section{Example Test Cases}
\begin{itemize}[leftmargin=1.2cm]
  \item Publish INFO log â†’ visible in service stream within SLA.
  \item Publish ERROR log â†’ alert created and notification fired.
  \item Kafka broker disruption â†’ logs buffered and delivered after recovery.
\end{itemize}

\section{Validation Metrics}
We tracked ingestion latency, delivery success rate, and dashboard query times. See Performance chapter for results.

\chapter{Monitoring and Alerting}
Dashboards visualize total logs, error and warning counts, level distribution, timelines, response time metrics, and top sources. Alerts trigger on ERROR logs per service. Operations can tune the ranges and thresholds based on SLOs.

\chapter{DevOps and CI/CD}
\section{Branching and Environments}
A typical branching strategy separates long-lived branches (\texttt{main} for production, \texttt{develop} for staging) and feature branches. Docker images are tagged per commit and environment. Configuration overlays control environment-specific settings.

\section{Build and Test Pipeline}
The CI pipeline builds Java services with Maven, runs unit tests, and publishes Docker images. A smoke test step verifies service health and log ingestion paths. Promotion to staging/production requires manual approval and change records.

\section{Infrastructure as Code}
Although this project uses Docker Compose for local orchestration, a production environment should leverage IaC (Terraform, Bicep) and GitOps practices for reproducible deployments, audit trails, and rollbacks.

\chapter{Data Management and Retention}
\section{Index Lifecycle and Retention Policies}
OpenSearch index policies should enforce retention windows and rollover strategies to control storage cost and query performance. Templates can shard and replicate appropriately per index.

\section{Field Mapping and Storage Efficiency}
Mapping frequently queried fields (e.g., \texttt{service}, \texttt{level}, \texttt{timestamp}) enables efficient aggregations. Avoid logging sensitive or high-cardinality fields unnecessarily to reduce storage and improve performance.

\section{Backup and Restore}
Regular snapshots of OpenSearch and backups of MongoDB (Graylog metadata) are required for disaster recovery. Test restore procedures periodically.

\chapter{SLOs, SLIs, and Alert Strategy}
\section{Service Level Indicators}
\begin{itemize}[leftmargin=1.2cm]
  \item Ingestion latency (service to searchable in Graylog).
  \item Log delivery success rate (messages delivered vs. produced).
  \item Query latency for common dashboards.
\end{itemize}

\section{Service Level Objectives}
Example SLOs: \textit{p95 ingestion latency < 2s}, \textit{delivery success rate > 99.9\%}, \textit{p95 dashboard query latency < 3s}. Alerts should be multi-window and noise-controlled.

\chapter{Risk Management}
\begin{table}[H]
\centering
\begin{tabular}{p{3.5cm} p{6.5cm} p{4.5cm}}
\toprule
\textbf{Risk} & \textbf{Impact} & \textbf{Mitigation} \\
\midrule
Kafka outage & Log delivery delayed & Buffering, retries, multi-broker cluster \\
OpenSearch overload & Slow queries / dropped indexing & Sharding, resource scaling, index lifecycle \\
PII in logs & Compliance violation & Redaction at source, validation, policies \\
Credential leakage & Security incident & Secrets management, least privilege \\
\bottomrule
\end{tabular}
\caption{Key risks and mitigations}
\end{table}

\chapter{Capacity and Cost Planning}
Estimate log volume (events/sec), average event size, daily ingestion (GB/day), retention window, and query concurrency. Use these to size OpenSearch nodes (CPU, RAM, storage IOPS) and Kafka partitions/throughput.

\chapter{Incident Response and Runbooks}
\section{Operational Runbooks}
\begin{itemize}[leftmargin=1.2cm]
  \item Graylog unreachable: check container, API readiness, ports 9000/12201.
  \item Kafka publish failures: verify broker health, topic existence, client configs.
  \item High query latency: review OpenSearch health, index size, slow logs.
\end{itemize}
Post-incident reviews feed improvements to dashboards, alerts, and capacity plans.

\chapter{Multi-Environment Configuration}
Use environment variables and separate configuration files for dev/stage/prod. Consider \texttt{docker-compose.override.yml} for local overrides and distinct Graylog inputs or streams per environment to avoid cross-contamination.

\chapter{Bill of Materials (BoM)}
\begin{table}[H]
\centering
\begin{tabular}{llp{7cm}}
\toprule
\textbf{Component} & \textbf{Version} & \textbf{Notes} \\
\midrule
Spring Boot & 3.5.3 & Backend framework for services \\
Java & 21 & LTS runtime \\
Graylog & 6.3 & Log management platform \\
OpenSearch & 2.x & Search and analytics backend \\
MongoDB & 6.0.5 & Graylog metadata storage \\
Kafka & wurstmeister/kafka & Distributed message broker \\
Zookeeper & 7.4.3 & Kafka coordination \\
logstash-logback-encoder & 7.3 & JSON encoding for Logback \\
logback-kafka-appender & 0.2.0-RC2 & Kafka appender (service1) \\
logback-gelf & 4.0.2 & GELF TCP appender (log-service) \\
gelfclient & 1.5.1 & GELF client library \\
Docker Compose & v2+ & Orchestration for local env \\
\bottomrule
\end{tabular}
\caption{Bill of Materials (as used in this project)}
\end{table}

\chapter{Ethical and Legal Considerations}
Avoid logging personal data (PII) and secrets. Apply data minimization, masking, and retention aligned with regulations (e.g., GDPR). Ensure access is logged and roles follow the principle of least privilege.

\chapter{Security and Compliance}\label{chap:security}
Security considerations include access control to Graylog, secure transport in production (TLS for GELF and Graylog UI), Kafka ACLs for producers/consumers, and secret management for credentials. For compliance, log retention and data minimization policies can be enforced via index settings.

\chapter{Performance and Scalability}
Kafka decouples producers and consumers, allowing the system to scale independently. Throughput can be increased via topic partitioning and consumer groups. GELF TCP batching and reconnect settings are tuned in Logback to handle transient failures. OpenSearch resources (heap, CPU, I/O) should be sized for indexing rates and query patterns.

\section{Throughput Estimation}
If $R$ is the average event rate (events/s) and $S$ the average size (bytes), the daily ingestion volume is $V = R \times S \times 86400$ bytes/day. With replication factor $\rho$ and overhead factor $\alpha$, storage needs approximate $V' = V \times \rho \times \alpha$.

\section{Partitioning Strategy}
Choose Kafka partitions $P$ to satisfy consumer parallelism and throughput: $P \geq N_c$ (number of consumer instances). Measure processing time per message to compute required parallelism.

\chapter{Testing and Validation}
Integration tests validated end-to-end flow: HTTP ingestion at services, Kafka delivery, consumer processing, and visibility in Graylog. Additional tests included error-level alerts and dashboard widget correctness. Synthetic load tests assessed latency and throughput across components.

\section{Threats to Validity}
Internal validity threats include environment-specific behavior in Docker Compose vs. production. External validity threats involve generalizing performance results to different workloads. Mitigations include parameterized tests and workload modeling.

\chapter{Troubleshooting and Operations}
Common issues involve container startup order, Kafka connectivity, and Graylog API availability. The init script includes retries and readiness checks. Operators can use container logs and Graylog search to diagnose issues quickly.

\chapter{Results and Discussion}
The platform centralizes logs across services, reduces mean time to resolution through actionable dashboards and alerts, and establishes a foundation for broader observability. Automation minimizes manual steps and configuration drift.

\chapter{Project Management}
\section{Organization and Roles}
The project was executed by the intern (Idris SADDI) under the supervision of Salma SEDDIK. Stakeholders included SRE and backend teams. Weekly checkpoints ensured alignment with objectives and timely risk handling.

\section{Schedule Tracking}
The Gantt plan was updated as tasks progressed. Buffer periods were reserved for stabilization and tuning. Changes were documented in commit messages and issue threads.

\section{Communication}
Stand-ups and async updates (issue comments, pull request notes) maintained transparency. Decisions were captured near the code (README and scripts) for future maintainers.

\chapter{Budget and Resource Estimate}
\begin{table}[H]
\centering
\begin{tabular}{p{5cm} p{4cm} p{6cm}}
\toprule
\textbf{Item} & \textbf{Cost (indicative)} & \textbf{Notes} \\
\midrule
Compute (containers) & Internal & Developer workstation resources for local runs \\
Storage (OpenSearch) & Internal & Depends on retention and volume; see capacity planning \\
Time (engineering) & Internship duration & Design, implementation, validation, documentation \\
\bottomrule
\end{tabular}
\caption{Budget/resource estimation for prototype environment}
\end{table}

\chapter{Knowledge Transfer and Maintainability}
\section{Documentation}
The repository README details architecture, setup, testing, and troubleshooting. Scripts include inline documentation and self-checks. The reportâ€™s appendices embed exact source files for traceability.

\section{Handover}
Handover sessions cover environment bootstrap, Graylog automation flows, and dashboard customization. Future contributors can extend services and alert definitions following established patterns.

\chapter{Evaluation and Academic Criteria}
This work demonstrates: problem framing (observability challenges), method (iterative delivery), engineering rigor (automation, testing), and reflection (risks, ethics, and future work). The deliverable meets academic expectations for completeness, reproducibility, and clarity.

\chapter*{General Conclusion}
\addcontentsline{toc}{chapter}{General Conclusion}
This internship consolidated my skills in distributed architectures, messaging, observability, and containerized deployments. The delivered solution provides a robust, industrializable foundation for microservices monitoring. Future enhancements include advanced correlation, distributed tracing (OpenTelemetry), SSO for Graylog, and richer alert routing.

\chapter*{Future Work}
\addcontentsline{toc}{chapter}{Future Work}
Planned improvements: secure-by-default TLS across services, CI/CD integration for infrastructure changes, automated capacity planning for OpenSearch, and multi-environment (dev/stage/prod) configuration overlays.

\chapter*{Glossary and Acronyms}
\addcontentsline{toc}{chapter}{Glossary and Acronyms}
\begin{description}
  \item[GELF] Graylog Extended Log Format
  \item[MDC] Mapped Diagnostic Context
  \item[SLI/SLO] Service Level Indicator / Objective
  \item[SRE] Site Reliability Engineering
  \item[IaC] Infrastructure as Code
\end{description}

\appendix
\chapter{Appendix A: docker-compose.yml}
\VerbatimInput[frame=single, fontsize=\scriptsize, xleftmargin=0.5cm, xrightmargin=0.5cm]{../docker-compose.yml}

\chapter{Appendix B: graylog-init.sh}
\VerbatimInput[frame=single, fontsize=\scriptsize, xleftmargin=0.5cm, xrightmargin=0.5cm]{../graylog-init.sh}

\chapter{Appendix C: service1 LogController.java}
\VerbatimInput[frame=single, fontsize=\scriptsize, xleftmargin=0.5cm, xrightmargin=0.5cm]{../service1/src/main/java/com/idris/service1/LogController.java}

\chapter{Appendix D: log-service LogConsumer.java}
\VerbatimInput[frame=single, fontsize=\scriptsize, xleftmargin=0.5cm, xrightmargin=0.5cm]{../log-service/src/main/java/com/idris/log_service/kafka/LogConsumer.java}

\chapter{Appendix E: logback-spring.xml (log-service)}
\VerbatimInput[frame=single, fontsize=\scriptsize, xleftmargin=0.5cm, xrightmargin=0.5cm]{../log-service/src/main/resources/logback-spring.xml}

\chapter{Appendix F: service1 application.properties}
\VerbatimInput[frame=single, fontsize=\scriptsize, xleftmargin=0.5cm, xrightmargin=0.5cm]{../service1/src/main/resources/application.properties}

\chapter{Appendix G: service2 application.properties}
\VerbatimInput[frame=single, fontsize=\scriptsize, xleftmargin=0.5cm, xrightmargin=0.5cm]{../service2/src/main/resources/application.properties}

\chapter{Appendix H: service2 logback-spring.xml}
\VerbatimInput[frame=single, fontsize=\scriptsize, xleftmargin=0.5cm, xrightmargin=0.5cm]{../service2/src/main/resources/logback-spring.xml}

\chapter{Appendix I: log-service application.properties}
\VerbatimInput[frame=single, fontsize=\scriptsize, xleftmargin=0.5cm, xrightmargin=0.5cm]{../log-service/src/main/resources/application.properties}

\chapter{Appendix J: service1 logback-spring.xml}
\VerbatimInput[frame=single, fontsize=\scriptsize, xleftmargin=0.5cm, xrightmargin=0.5cm]{../service1/src/main/resources/logback-spring.xml}

\chapter*{References}
\addcontentsline{toc}{chapter}{References}
\begin{thebibliography}{9}
\bibitem{graylog-doc} Graylog Documentation, \url{https://docs.graylog.org/}, accessed 09/23/2025.
\bibitem{kafka-doc} Apache Kafka Documentation, \url{https://kafka.apache.org/documentation/}, accessed 09/23/2025.
\bibitem{spring-doc} Spring Boot Reference, \url{https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/}, accessed 09/23/2025.
\bibitem{opensearch-doc} OpenSearch Documentation, \url{https://opensearch.org/docs/}, accessed 09/23/2025.
\bibitem{logback-gelf} logback-gelf, \url{https://github.com/mp911de/logstash-gelf}, accessed 09/23/2025.
\end{thebibliography}

\end{document}
