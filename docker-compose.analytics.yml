version: '3.8'

# Log Analytics and ML Pipeline
networks:
  analytics:
    driver: bridge

volumes:
  jupyter_data:
  mlflow_data:

services:
  # Jupyter Notebook for log analysis
  jupyter:
    image: jupyter/datascience-notebook:latest
    container_name: jupyter-analytics
    ports:
      - "8888:8888"
    volumes:
      - jupyter_data:/home/jovyan/work
      - ./analytics/notebooks:/home/jovyan/work/notebooks
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=loganalytics
    networks:
      - analytics
      - graynet

  # MLflow for ML model management
  mlflow:
    image: python:3.11-slim
    container_name: mlflow-server
    ports:
      - "5000:5000"
    volumes:
      - mlflow_data:/mlflow
      - ./analytics/mlflow:/app
    working_dir: /app
    command: >
      bash -c "
        pip install mlflow[extras] &&
        mlflow server 
        --backend-store-uri sqlite:///mlflow/mlflow.db 
        --default-artifact-root /mlflow/artifacts 
        --host 0.0.0.0 
        --port 5000
      "
    networks:
      - analytics

  # Apache Spark for big data processing
  spark-master:
    image: bitnami/spark:3.4
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - analytics

  spark-worker:
    image: bitnami/spark:3.4
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master
    networks:
      - analytics

  # Apache Airflow for ML pipeline orchestration
  airflow:
    image: apache/airflow:2.7.1
    container_name: airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__FERNET_KEY=YWJjZGVmZ2hpams=
      - AIRFLOW__WEBSERVER__SECRET_KEY=secret
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    ports:
      - "8081:8080"
    volumes:
      - ./analytics/airflow/dags:/opt/airflow/dags
      - ./analytics/airflow/plugins:/opt/airflow/plugins
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin &&
        airflow webserver
      "
    networks:
      - analytics
